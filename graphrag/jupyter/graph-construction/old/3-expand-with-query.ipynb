{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Expand Context with Hypothetical Questions\n",
    "\n",
    "The Chunk Nodes are individual chunks of text that used\n",
    "to be part of an entire document.\n",
    "\n",
    "You can reconstruct the original context by connecting the nodes\n",
    "with relationships. It also makes the data easy to navigate and understand.\n",
    "\n",
    "That is super helpful when you're building an application, for debugging and testing.\n",
    "\n",
    "And, you can provide a better user experience. Your users will be able \n",
    "to directly interact with the data and even provide feedback that will\n",
    "improve subsequent answers. \n",
    "\n",
    "You will create a connected context by making the following\n",
    "changes to the knowledge graph:\n",
    "\n",
    "1. Extract, create `(:Document)` nodes for each original source Form.\n",
    "2. Enhance, add a summarized text property to each `(:Form)` node.\n",
    "3. Expand, connect each `(:Chunk)` to the `(:Form)` node that it is part of\n",
    "\n",
    "The graph will look like this...\n",
    "\n",
    "```cypher\n",
    "(:Question\n",
    "  questionId: string\n",
    "  question: string\n",
    "  embedding: float[] // vector embedding of quesion\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "```cypher\n",
    "// Section has Question\n",
    "(:Section)-[:HAS_QUESTIONS]->(:Question)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n",
      "Connecting to Neo4j at bolt://neo4j-1:7687 as neo4j\n",
      "Using data from /home/jovyan/data/single\n",
      "Embedding with ollama using mxbai-embed-large\n",
      "Chatting with ollama using llama3\n"
     ]
    }
   ],
   "source": [
    "%run 'shared.ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhance - create hypotetical question nodes, with embedding\n",
    "\n",
    "During the file processing above, the text from all interesting\n",
    "items were added to the `fullText` property of the `all_forms` dictionaries.\n",
    "\n",
    "We'll use an LLM to generate the questions and create an embedding.\n",
    "\n",
    "Both the text summary and the embdding will be added to the Form nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What is the total area of the Goalkeeper Restricted Area?', 'How many meters longer is one side of the trapezoid compared to the other?', \"If a player's skate extends 0.1m beyond the red line, will it be considered out of bounds?\"]\n"
     ]
    }
   ],
   "source": [
    "text = \"The Goalkeeper Restricted Area is a trapezoidal zone marked behind each goal on the ice surface, defined by two red lines that measure 6.80m along the goal line and 8.60m along the boards\"\n",
    "result = chat_api.invoke(\n",
    "      f\"\"\"Generate a maximum of 3 short hypothetical questions based on the information from the text\\n {text}. Only respond with the questions delimited by a newline (\\n) and do not return \"Here are some hypothetical questions based on the information:\" \n",
    "      \"\"\")\n",
    "print (result.content.split('\\n\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text embeddings will have 1024 dimensions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<Record id=12 name='chunks_vector' state='ONLINE' populationPercent=100.0 type='VECTOR' entityType='NODE' labelsOrTypes=['Chunk'] properties=['embedding'] indexProvider='vector-2.0' owningConstraint=None lastRead=neo4j.time.DateTime(2024, 6, 2, 18, 0, 56, 401000000, tzinfo=<UTC>) readCount=5>,\n",
       " <Record id=2 name='questions_vector' state='POPULATING' populationPercent=0.0 type='VECTOR' entityType='NODE' labelsOrTypes=['Question'] properties=['embedding'] indexProvider='vector-2.0' owningConstraint=None lastRead=None readCount=None>,\n",
       " <Record id=6 name='sections_vector' state='ONLINE' populationPercent=100.0 type='VECTOR' entityType='NODE' labelsOrTypes=['Section'] properties=['summaryEmbedding'] indexProvider='vector-2.0' owningConstraint=None lastRead=neo4j.time.DateTime(2024, 6, 2, 17, 46, 42, 478000000, tzinfo=<UTC>) readCount=1>]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an embedding to find out the dimensions of the vector\n",
    "text_embedding = embeddings_api.embed_query(\"embed this text using an LLM\")\n",
    "vector_dimensions = len(text_embedding) \n",
    "\n",
    "print(f\"Text embeddings will have {vector_dimensions} dimensions\")\n",
    "# Create a vector index called \"sections_vector\" the `summaryEmbedding`` property of nodes labeled `Section`. \n",
    "gdb.execute_query(\"\"\"\n",
    "         CREATE VECTOR INDEX `questions_vector` IF NOT EXISTS\n",
    "          FOR (s:Question) ON (s.embedding) \n",
    "          OPTIONS { indexConfig: {\n",
    "            `vector.dimensions`: $vectorDimensionsParam,\n",
    "            `vector.similarity_function`: 'cosine'    \n",
    "         }}\n",
    "\"\"\",\n",
    "  vectorDimensionsParam=vector_dimensions\n",
    ")\n",
    "\n",
    "# Check the vector indexes in the graph\n",
    "gdb.execute_query('SHOW VECTOR INDEXES').records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\Inserting Question node with question and embedding link it to Section with ID ca4f9dcf204e2037bfe5884867bead98bd9cbaf8 ...\n",
      "\\Inserting Question node with question and embedding link it to Section with ID ca4f9dcf204e2037bfe5884867bead98bd9cbaf8 ...\n",
      "\\Inserting Question node with question and embedding link it to Section with ID ca4f9dcf204e2037bfe5884867bead98bd9cbaf8 ...\n",
      "\\Inserting Question node with question and embedding link it to Section with ID 70669d65799f0b85f53fa4d87f745b1ec129af58 ...\n",
      "\\Inserting Question node with question and embedding link it to Section with ID 70669d65799f0b85f53fa4d87f745b1ec129af58 ...\n",
      "\\Inserting Question node with question and embedding link it to Section with ID 70669d65799f0b85f53fa4d87f745b1ec129af58 ...\n",
      "\\Inserting Question node with question and embedding link it to Section with ID 3256ff621b0e47207f42b2db33a4589ec1ddd458 ...\n",
      "\\Inserting Question node with question and embedding link it to Section with ID 3256ff621b0e47207f42b2db33a4589ec1ddd458 ...\n",
      "\\Inserting Question node with question and embedding link it to Section with ID 3256ff621b0e47207f42b2db33a4589ec1ddd458 ...\n",
      "\\Inserting Question node with question and embedding link it to Section with ID b9ea9b87e07555afd018b9a39b5c3e4180414475 ...\n",
      "\\Inserting Question node with question and embedding link it to Section with ID b9ea9b87e07555afd018b9a39b5c3e4180414475 ...\n",
      "\\Inserting Question node with question and embedding link it to Section with ID b9ea9b87e07555afd018b9a39b5c3e4180414475 ...\n",
      "\\Inserting Question node with question and embedding link it to Section with ID c31a58e8aebfd957ed100bf860f367a0ca7c37de ...\n",
      "\\Inserting Question node with question and embedding link it to Section with ID c31a58e8aebfd957ed100bf860f367a0ca7c37de ...\n",
      "\\Inserting Question node with question and embedding link it to Section with ID c31a58e8aebfd957ed100bf860f367a0ca7c37de ...\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import uuid\n",
    "\n",
    "query = \"\"\"\n",
    "MATCH (s:Section)-[:CONTAINS]->(c:Chunk)\n",
    "WITH s, collect(c.text) as textList\n",
    "RETURN s.sectionId AS sectionId, s.level AS level, apoc.text.join(textList, \" \\n \") AS text\n",
    "\"\"\"\n",
    "\n",
    "sections_with_text = gdb.execute_query(query).records\n",
    "for section in sections_with_text[0:]:\n",
    "    question_info = {}\n",
    "    text = section[\"text\"]\n",
    "    result = chat_api.invoke(f\"\"\"\n",
    "                Generate a maximum of 3 short hypothetical questions based on the information from the text\\n {text}. Only respond with the questions delimited by a newline (\\n) and do not return \"Here are some hypothetical questions based on the information:\n",
    "                \"\"\" \n",
    "    )    \n",
    "    questions = result.content.split('\\n\\n')\n",
    "    for question in questions:\n",
    "        question_info['question'] = question\n",
    "        question_info['sectionId'] = section[\"sectionId\"]\n",
    "        question_info['questionId'] = str(uuid.uuid4())\n",
    "    \n",
    "        question_embedding = embeddings_api.embed_query(question)\n",
    "        question_info['embedding'] = question_embedding\n",
    "        print(f\"\\Inserting Question node with question and embedding link it to Section with ID {question_info['sectionId']} ...\")\n",
    "\n",
    "        gdb.execute_query(\"\"\"\n",
    "          MERGE (q:Question {questionId: $questionInfoParam.questionId} )\n",
    "            SET q.question = $questionInfoParam.question \n",
    "          WITH q\n",
    "            CALL db.create.setNodeVectorProperty(q, \"embedding\", $questionInfoParam.embedding)\n",
    "          MATCH (s:Section {sectionId: $questionInfoParam.sectionId} )\n",
    "          MERGE (s)-[r:HAS_QUESTION]->(q)\n",
    "        \"\"\", \n",
    "        questionInfoParam=question_info\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c751aae4-dfff-4f5f-ab8a-ddea14154037\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "uuid4 = uuid.uuid4()\n",
    "print(str(uuid4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
