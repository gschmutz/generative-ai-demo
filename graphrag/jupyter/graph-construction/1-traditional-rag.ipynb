{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Traditional RAG with Neo4J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are using tradtional RAG strategy consisting of `Chunk` nodes arranged into linked lists.\n",
    "\n",
    "\n",
    "1. Extract text from Markdown document, split text into chunks, create `Chunk` nodes\n",
    "2. Enhance each `Chunk` node with a text embedding\n",
    "3. Expand the `Chunk` nodes with `NEXT` relationships to form linked lists\n",
    "\n",
    "```cypher\n",
    "(:Chunk \n",
    "  chunkId: string\n",
    "  text: string\n",
    "  header1: string\n",
    "  header2: string\n",
    "  header3: string\n",
    "  header4: string\n",
    "  path: string\n",
    "  documentUri: string\n",
    "  ebmbedding: float[]\n",
    ")\n",
    "```\n",
    "\n",
    "```cypher\n",
    "(:Chunk)-[:NEXT]->(:Chunk)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import some python packages, set up global constants, and create a connection to the Neo4j database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n",
      "#*****************************************************************\n",
      "# Neo4j\n",
      "#*****************************************************************\n",
      "NEO4J_URI=bolt://neo4j-1:7687\n",
      "NEO4J_USERNAME=neo4j\n",
      "NEO4J_PASSWORD=abc123abc123\n",
      "NEO4J_DATABASE=neo4j\n",
      "\n",
      "# either ollama or openai\n",
      "EMBEDDING_API=ollama\n",
      "EMBEDDING_MODEL=mxbai-embed-large\n",
      "# either ollama or openai\n",
      "CHAT_API=ollama\n",
      "CHAT_MODEL=llama3\n",
      "\n",
      "#OLLAMA_URL=http://192.168.1.102:11434\n",
      "#OLLAMA_URL=http://172.20.10.2:11434\n",
      "OLLAMA_URL=http://host.docker.internal:11434\n",
      "OPEN_API_KEY=\n",
      "\n",
      "DATA_DIR=\n",
      "\n",
      "Connecting to Neo4j at bolt://neo4j-1:7687 as neo4j\n",
      "Embedding with ollama using mxbai-embed-large\n",
      "Chatting with ollama using llama3\n"
     ]
    }
   ],
   "source": [
    "%run 'shared.ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare a GraphDatabase interface\n",
    "\n",
    "You will use the Neo4j `GraphDatabase` interface to send queries to the Neo4j database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, World!'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expect `gdb` to be defined in the shared notebook\n",
    "# gdb = GraphDatabase.driver(uri=NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "\n",
    "result = gdb.execute_query(\"RETURN 'Hello, World!' AS message\")\n",
    "\n",
    "result.records[0].get('message')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input data pre-preprocessing\n",
    "\n",
    "The IIHF pdf document we will be working with has been preprocessed from the original source into markdown. We will use the markdown here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step by step inspection of the document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start with the file\n",
    "\n",
    "Get the the file name and then loading the markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data-transfer/mdp-demo/bundesgesetz-krankenversicherung.md\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#data_folder = \"/data-transfer/iihf\"\n",
    "data_folder = \"/data-transfer/mdp-demo\"\n",
    "\n",
    "loader = DirectoryLoader(data_folder, glob=\"**/*.md\", loader_cls=TextLoader)\n",
    "documents = loader.load()\n",
    "\n",
    "print (documents[0].metadata[\"source\"])\n",
    "print (len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text splitter from Langchain\n",
    "\n",
    "We can use a text splitter function from Langchain.\n",
    "\n",
    "The `RecursiveCharacterTextSplitter` will use newlines\n",
    "and then whitespace characters to break down a text until\n",
    "the chunks are small enough. This strategy is generally\n",
    "good at keeping paragraphs together.\n",
    "\n",
    "Set a chunk size of 600 characters,\n",
    "with 0 characters of overlap between each chunk,\n",
    "using the built-in `len` function to calculate the \n",
    "text length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting text into chunks using the RecursiveCharacterTextSplitter \n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 600,\n",
    "    chunk_overlap  = 0,\n",
    "    length_function = len,\n",
    "    is_separator_regex = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text splitter demonstration\n",
    "\n",
    "You can see what the text splitter will do by splitting up\n",
    "the `page_content`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6 Wenn der bisherige Versicherer den Wechsel des Versicherers verunmöglicht, hat er der versicherten Person den daraus entstandenen Schaden zu ersetzen, insbeson\\xaddere die Prämiendifferenz.<sup>[\\\\[34\\\\]](##footnote-35)</sup>\\n\\n7 Der bisherige Versicherer darf eine versicherte Person nicht dazu zwingen, bei einem Wechsel des Versicherers auch die bei ihm abgeschlossenen Zusatzversiche\\xadrungen im Sinne von Artikel 2 Absatz 2 KVAG zu kündigen.<sup>[\\\\[35\\\\]](##footnote-36)</sup>'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks = text_splitter.split_text(documents[0].page_content)\n",
    "text_chunks[19]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markdown Header Text Splitter combined Recursive Character Text Splitter from Langchain\n",
    "\n",
    "\n",
    "#### Markdown Header Text Splitter\n",
    "We first use the Markdown Header Text splitter to split on the structure of the markdown document (using Header 1 - 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data-transfer/mdp-demo/bundesgesetz-krankenversicherung.md\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print (documents[0].metadata[\"source\"])\n",
    "print (len(documents))\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"header1\"),\n",
    "    (\"##\", \"header2\"),\n",
    "    (\"###\", \"header3\"),\n",
    "    (\"####\", \"header4\"),\n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=True)\n",
    "md_header_splits = markdown_splitter.split_text(documents[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='(KVG)  \\nvom 18. März 1994 (Stand am 1. Juli 2025)  \\nDie Bundesversammlung der Schweizerischen Eidgenossenschaft,  \\ngestützt auf Artikel 34bis der Bundesverfassung<sup>[\\\\[2\\\\]](##footnote-3)</sup>,<sup>[\\\\[3\\\\]](##footnote-4)</sup>\\nnach Einsicht in die Botschaft des Bundesrates vom 6. November 1991<sup>[\\\\[4\\\\]](##footnote-5)</sup>,  \\nbeschliesst:', metadata={'header1': 'Bundesgesetz über die Krankenversicherung'})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_header_splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Each Rink must have two (2) “Goal Nets”, one at either end of the Rink. The “Goal Net” is comprised of a Goal frame and netting. The open end of the goal net must face Center ice.  \\nEach Goal Net must be located in the center of the Goal Line at either end and must be installed in such manner as to remain stationary during the progress of the game. The Goal posts must be kept in position by means of flexible pegs affixed in the ice or floor, but which displace the Goal Net from its moorings upon significant contact.  \\nThe holes for the goal pegs must be located exactly on the Goal Line.  \\nThe Goal posts shall be of an approved design and material, extending vertically 1.22 m above the surface of the ice and set 1.83 m apart measured from the inside of the posts. A crossbar of the same material as the Goal posts shall extend from the top of one post to the top of the other. The Goal posts and crossbar shall be painted in red color and all other exterior surfaces shall be painted in white color.', metadata={'header1': 'IIHF Official Rulebook 2023/24', 'header2': 'SECTION 01 PLAYING AREA', 'header3': 'RULE 2 GOAL POSTS AND NETS', 'header4': '2.1 GOAL POSTS'})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_header_splits[11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recursive Character Text Splitter\n",
    "\n",
    "and now also use the Recursive Character Text Splitter to further split the text blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='(KVG)  \\nvom 18. März 1994 (Stand am 1. Juli 2025)  \\nDie Bundesversammlung der Schweizerischen Eidgenossenschaft,  \\ngestützt auf Artikel 34bis der Bundesverfassung<sup>[\\\\[2\\\\]](##footnote-3)</sup>,<sup>[\\\\[3\\\\]](##footnote-4)</sup>\\nnach Einsicht in die Botschaft des Bundesrates vom 6. November 1991<sup>[\\\\[4\\\\]](##footnote-5)</sup>,  \\nbeschliesst:', metadata={'header1': 'Bundesgesetz über die Krankenversicherung'})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Char-level splits\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size = 600\n",
    "chunk_overlap = 0\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    length_function = len,\n",
    "    is_separator_regex = False,    \n",
    ")\n",
    "\n",
    "# Split\n",
    "chunks = text_splitter.split_documents(md_header_splits)\n",
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='2 Bei der Mitteilung der neuen Prämie kann die versicherte Person den Versicherer unter Einhaltung einer einmonatigen Kündigungsfrist auf das Ende des Monats wechseln, welcher der Gültigkeit der neuen Prämie vorangeht. Der Versicherer muss die neuen, vom Bundesamt für Gesundheit<sup>[\\\\[29\\\\]](##footnote-30)</sup> (BAG)<sup>[\\\\[30\\\\]](##footnote-31)</sup> genehmigten Prämien jeder versicherten Person mindestens zwei Monate im Voraus mitteilen und dabei auf das Recht, den Versicherer zu wechseln, hinweisen.<sup>[\\\\[31\\\\]](##footnote-32)</sup>', metadata={'header1': 'Bundesgesetz über die Krankenversicherung', 'header2': '2\\\\. Titel: Obligatorische Krankenpflegeversicherung'})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "479"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a graph from the chunks\n",
    "\n",
    "You now have chunks prepared for creating a knowledge graph.\n",
    "\n",
    "The graph will have 1 node per chunk, containing the chunk text and metadata as properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge chunk query\n",
    "\n",
    "You will use a Cypher query to merge the chunks into the graph.\n",
    "\n",
    "This query accepts a query parameter called `chunkParam` which is expected\n",
    "to have the data record containing the chunk and metadata.\n",
    "\n",
    "The `MERGE` query will first match an existing node with the same `chunkId` property.\n",
    "\n",
    "If no such node exists, it will create a new node and the `ON CREATE` clause will set the properties using values from the `chunkParam` query parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_chunk_node_query = \"\"\"\n",
    "MERGE(c:Chunk {chunkId: $chunkParam.chunkId})\n",
    "    ON CREATE SET \n",
    "        c.source = $chunkParam.source, \n",
    "        c.chunkSeqId = $chunkParam.chunkSeqId, \n",
    "        c.path = $chunkParam.path,\n",
    "        c.text = $chunkParam.text,\n",
    "        c.documentUri = $chunkParam.documentUri,\n",
    "        c += $chunkParam.metadata\n",
    "RETURN c\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to create nodes for all chunks.\n",
    "# This will use the `merge_chunk_node_query` to create a `:Chunk` node for each chunk.\n",
    "def create_chunk_id(metadata, idx) -> str:\n",
    "    id = metadata[\"header1\"]\n",
    "    if 'header2' in metadata:\n",
    "        id = id + '|' + metadata[\"header2\"]\n",
    "    if 'header3' in metadata:\n",
    "        id = id + '|' + metadata[\"header3\"]\n",
    "    if 'header4' in metadata:\n",
    "        id = id + '|' + metadata[\"header4\"]\n",
    "    id = id + '|' + str(idx)    \n",
    "    return hashlib.sha1(id.encode()).hexdigest()\n",
    "\n",
    "def create_path(metadata) -> str: \n",
    "    path = metadata[\"header1\"]\n",
    "#    if 'header2' in metadata:\n",
    "#        path = path + '/' + metadata[\"header2\"]\n",
    "#    if 'header3' in metadata:\n",
    "#        path = path + '/' + metadata[\"header3\"]\n",
    "#    if 'header4' in metadata:\n",
    "#        path = path + '/' + metadata[\"header4\"]\n",
    "    return path.replace(' ', '_').replace('.', '_').lower()\n",
    "\n",
    "def create_nodes_for_all_chunks(documentUri, chunks):\n",
    "    node_count = 0\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_id = create_chunk_id(chunk.metadata, i)\n",
    "        path = create_path(chunk.metadata)\n",
    "        gdb.execute_query(merge_chunk_node_query, \n",
    "                chunkParam = { \"chunkId\": chunk_id, \"source\":\"\", \"chunkSeqId\":i, \"text\": chunk.page_content, \"metadata\": chunk.metadata, \"path\": path, \"documentUri\": documentUri }\n",
    "        )\n",
    "        node_count += 1\n",
    "    print(f\"Created {node_count} nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare unique constraint\n",
    "\n",
    "Before calling the helper function to create a knowledge graph,\n",
    "we will take one extra step to make sure we don't duplicate data.\n",
    "\n",
    "The uniqueness constraint is also index. It's job is to ensure that\n",
    "a particular property is unique for all nodes that share a common label.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Record id=5 name='unique_chunk' type='UNIQUENESS' entityType='NODE' labelsOrTypes=['Chunk'] properties=['chunkId'] ownedIndex='unique_chunk' propertyType=None>]\n"
     ]
    }
   ],
   "source": [
    "# Create a uniqueness constraint on the chunkId property of Chunk nodes \n",
    "gdb.execute_query(\"\"\"\n",
    "CREATE CONSTRAINT unique_chunk IF NOT EXISTS \n",
    "    FOR (c:Chunk) REQUIRE c.chunkId IS UNIQUE\n",
    "\"\"\")\n",
    "\n",
    "created_indexes = gdb.execute_query('SHOW CONSTRAINTS').records\n",
    "print(created_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create index\n",
    "\n",
    "To speed up lookup on the \"path\" property, we create an index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Record id=1 name='index_2bc8b8e7' state='ONLINE' populationPercent=100.0 type='RANGE' entityType='NODE' labelsOrTypes=['Chunk'] properties=['path'] indexProvider='range-1.0' owningConstraint=None lastRead=neo4j.time.DateTime(2024, 6, 23, 18, 14, 43, 132000000, tzinfo=<UTC>) readCount=703>, <Record id=6 name='unique_chunk' state='ONLINE' populationPercent=100.0 type='RANGE' entityType='NODE' labelsOrTypes=['Chunk'] properties=['chunkId'] indexProvider='range-1.0' owningConstraint='unique_chunk' lastRead=None readCount=None>]\n"
     ]
    }
   ],
   "source": [
    "# Create a uniqueness constraint on the chunkId property of Chunk nodes \n",
    "gdb.execute_query(\"\"\"\n",
    "CREATE INDEX FOR (c:Chunk) ON (c.path)\n",
    "\"\"\")\n",
    "\n",
    "created_indexes = gdb.execute_query('SHOW INDEXES').records\n",
    "print(created_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all chunks\n",
    "\n",
    "Perform the node creation for all files in an import directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 479 nodes\n",
      "CPU times: user 274 ms, sys: 30.2 ms, total: 304 ms\n",
      "Wall time: 2.11 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "479"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "create_nodes_for_all_chunks(documents[0].metadata[\"source\"], chunks)\n",
    "\n",
    "# Check the number of nodes in the graph\n",
    "gdb.execute_query(\"MATCH (c:Chunk) RETURN count(c) as chunkCount\").records[0].get('chunkCount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Record uniqueHeader4Count=19>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of unique company CUSIPs (company IDs) in the graph\n",
    "# Expect this to match the `uniqueCompanyCount` from the previous cell\n",
    "gdb.execute_query(\"MATCH (c:Chunk) RETURN count(distinct(c.header4)) as uniqueHeader4Count\").records[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add vector embeddings for the text of each chunk  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "You will use the `embeddings_api` defined in `shared.ipynb` to get the vector embeddings \n",
    "for the text of each chunk. This api will use an LLM to calculate an embedding for text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3996089696884155, 0.040465839207172394, 0.30920103192329407, -0.07513697445392609] ....\n",
      "Text embeddings will have 1024 dimensions\n"
     ]
    }
   ],
   "source": [
    "# A simple example of how to use the embeddings API\n",
    "text_embedding = embeddings_api.embed_query(\"embed this text using an LLM\")\n",
    "\n",
    "print(f\"{text_embedding[0:4]} ....\")\n",
    "\n",
    "# all embeddings will have the same size, which is the dimensions of the vector\n",
    "vector_dimensions = len(text_embedding) \n",
    "\n",
    "print(f\"Text embeddings will have {vector_dimensions} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare a vector index\n",
    "\n",
    "Now that you have a graph populated with `Chunk` nodes, \n",
    "you can add vector embeddings.\n",
    "\n",
    "First, prepare a vector index to store the embeddings.\n",
    "\n",
    "The index will be called `chunks_vector` and will store\n",
    "embeddings for nodes labeled as `Chunk` in a property\n",
    "called `emedding`.\n",
    "\n",
    "The embeddings index will match the dimensions of the \n",
    "embeddings returned by the `embeddings_api` and will use \n",
    "the cosine similarity function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Record id=2 name='chunks_vector' state='ONLINE' populationPercent=100.0 type='VECTOR' entityType='NODE' labelsOrTypes=['Chunk'] properties=['embedding'] indexProvider='vector-2.0' owningConstraint=None lastRead=None readCount=0>]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a vector index called \"chunks_vector\" the `embedding`` property of nodes labeled `Chunk`. \n",
    "# neo4j_create_vector_index(kg, VECTOR_INDEX_NAME, 'Chunk', 'embedding')\n",
    "gdb.execute_query(\"\"\"\n",
    "         CREATE VECTOR INDEX `chunks_vector` IF NOT EXISTS\n",
    "          FOR (c:Chunk) ON (c.embedding) \n",
    "          OPTIONS { indexConfig: {\n",
    "            `vector.dimensions`: $vectorDimensionsParam,\n",
    "            `vector.similarity_function`: 'cosine'    \n",
    "         }}\n",
    "\"\"\",\n",
    "  vectorDimensionsParam = vector_dimensions\n",
    ")\n",
    "\n",
    "# Check the vector indexes in the graph\n",
    "gdb.execute_query('SHOW VECTOR INDEXES').records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create text embeddings\n",
    "\n",
    "Creating the text embeddings will be a two step process. \n",
    "\n",
    "First, collect all chunk text and chunk ids from the graph.\n",
    "Yes these are the same chunk ids that were used to create the graph\n",
    "and you could save time by doing this all at once. We're doing\n",
    "this incrementally to show the process, not optimized for speed.\n",
    "\n",
    "Next, use the `embeddings_api` to get the embeddings for the text\n",
    "and write those values back into the graph. \n",
    "\n",
    "This will take some time to run as we're doing it one chunk at a time,\n",
    "calling out to the `embeddings_api` for each then writing all those\n",
    "results back into the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding all chunks that need embedding...\n",
      "Generating vector embeddings, then writing into each chunk...\n",
      "CPU times: user 3.21 s, sys: 317 ms, total: 3.52 s\n",
      "Wall time: 44.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def text_for_embedding(text, header2, header3, header4) -> str:\n",
    "    text_for_embed = text;\n",
    "    if header4 is not None:\n",
    "        text_for_embed = header4 + '>>' + text_for_embed\n",
    "    return text_for_embed;\n",
    "\n",
    "# Create vector embeddings for all the Chunk text, in batches.\n",
    "# Use this for larger number of chunks so that the query\n",
    "# can be re-run without losing all progress\n",
    "print(\"Finding all chunks that need embedding...\")\n",
    "all_chunks_for_embed = gdb.execute_query(\"\"\"\n",
    "  MATCH (chunk:Chunk) WHERE chunk.embedding IS NULL\n",
    "  RETURN chunk.text AS text, chunk.header2 as header2, chunk.header3 as header3, chunk.header4 as header4, chunk.chunkId AS chunkId\n",
    "  \"\"\").records\n",
    "\n",
    "print(\"Generating vector embeddings, then writing into each chunk...\")\n",
    "for chunk in all_chunks_for_embed:\n",
    "  text = text_for_embedding(chunk['text'], chunk['header2'], chunk['header3'], chunk['header4'])\n",
    "  #print (text)\n",
    "  embedding = embeddings_api.embed_query(text)\n",
    "  gdb.execute_query(\"\"\"\n",
    "    MATCH (chunk:Chunk {chunkId: $chunkIdParam})\n",
    "    CALL db.create.setNodeVectorProperty(chunk, \"embedding\", $embeddingParam)    \n",
    "    \"\"\", \n",
    "    chunkIdParam=chunk['chunkId'], embeddingParam=embedding\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This next cell is an alternative to create embeddings and the vector index in one by using Langchain**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Using Langchain to create the vector index (alternative to the previous cell, switch to code to run it)\n",
    "from langchain.vectorstores import Neo4jVector\n",
    "Neo4jVector.from_existing_graph(\n",
    "    embedding=embeddings_api,\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,\n",
    "    database=NEO4J_DATABASE,\n",
    "    index_name='chunks_vector',\n",
    "    node_label=\"Chunk\",\n",
    "    text_node_properties=['text'],\n",
    "    embedding_node_property='embedding',\n",
    ")\n",
    "\n",
    "# Check the vector indexes in the graph\n",
    "gdb.execute_query('SHOW VECTOR INDEXES').records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example questions - vector similarity search with Neo4j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Neo4j vector search helper\n",
    "\n",
    "The `shared.ipynb` notebook has a helper function to perform a vector similarity search\n",
    "using the Neo4j Knowledge Graph.\n",
    "\n",
    "It will perform vector similarity search using the `chunks_vector` vector index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = neo4j_vector_search(\n",
    "#    'what is the size of the rink', VECTOR_INDEX_NAME\n",
    "    'what happens if player is injured', VECTOR_INDEX_NAME\n",
    ")\n",
    "search_results[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Answering chat with Langchain \n",
    "\n",
    "Notice that we only performed vector search. So what we're getting\n",
    "back is the raw chunk text.\n",
    "\n",
    "If we want to create a chatbot that provides actual answers to\n",
    "a question, we can build a RAG system using Langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the context of a fantasy sports league, if a player is injured, it can have various effects on their performance and your team\\'s overall success. Here are some possible scenarios:\\n\\n1. **Injury Report**: The fantasy platform or league will typically provide an injury report, which indicates the severity of the injury and the expected recovery time.\\n2. **Out for the Season**: If a player is severely injured and out for the season, they may be placed on the \"injured reserve\" (IR) list. This means they won\\'t contribute to your team\\'s scoring for the remainder of the season.\\n3. **Short-Term Absence**: For less severe injuries, a player might miss only a few games or weeks before returning to action. In this case, you can continue to start them in your lineup once they\\'re healthy enough to play.\\n4. **Questionable Status**: Some players may be listed as \"questionable\" for a game, indicating that their status is uncertain due to the injury. You might need to make an informed decision about whether to start them or not.\\n5. **Waiver Wire Pickups**: If a player is injured and out for an extended period, you might consider picking up a replacement on the waiver wire (if available). This can help minimize the impact of their absence on your team\\'s performance.\\n\\nTo mitigate the effects of an injured player:\\n\\n1. **Keep an Eye on Injury Reports**: Monitor injury reports regularly to stay informed about the status of your players.\\n2. **Have a Solid Bench**: Make sure you have a strong bench with capable players who can fill in for an injured starter.\\n3. **Make Strategic Moves**: Consider making trades or pickups to strengthen your team and minimize the impact of an injured player.\\n4. **Adjust Your Lineup**: Be prepared to adjust your lineup accordingly, taking into account the severity of the injury and the expected recovery time.\\n\\nRemember, injuries are a natural part of any fantasy sports league. By being proactive and adaptable, you can minimize their impact on your team\\'s success.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try the chat api directly\n",
    "result = chat_api.invoke(\"what happens if player is injured\")\n",
    "#result = chat_api.invoke(\"what is the size of the rink\")\n",
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neo4j Vector Store\n",
    "\n",
    "The easiest way to start using Neo4j with Langchain is with the `Neo4jVector` interface. This makes Neo4j look like a vector store using\n",
    "the vector index you created earlier.\n",
    "\n",
    "Under the hood, it will use the Cypher language for performing vector similarity searches.\n",
    "\n",
    "The configuration specifies a few important things:\n",
    "- use the defined `embeddings_api` for embeddings\n",
    "- how to connect to the Neo4j database\n",
    "- the name of the vector index to use\n",
    "- the label of the nodes to search\n",
    "- the property name of the text on those nodes\n",
    "- and, the property name of the embeddings on those nodes\n",
    "\n",
    "That vector store then gets converted into a retriever\n",
    "and finally added to a Question Answering chain. \n",
    "\n",
    "Prompt is retrieved from: <https://smith.langchain.com/hub/rlm/rag-prompt-llama>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a langchain vector store from the existing Neo4j knowledge graph.\n",
    "neo4j_vector_store = Neo4jVector.from_existing_graph(\n",
    "    embedding=embeddings_api,\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,\n",
    "    index_name=VECTOR_INDEX_NAME,\n",
    "    node_label=VECTOR_NODE_LABEL,\n",
    "    text_node_properties=[VECTOR_SOURCE_PROPERTY],\n",
    "    embedding_node_property=VECTOR_EMBEDDING_PROPERTY,\n",
    ")\n",
    "\n",
    "# RAG prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt-llama\")\n",
    "\n",
    "# Create a retriever from the vector store\n",
    "retriever = neo4j_vector_store.as_retriever(search_kwargs={'k': 1})\n",
    "\n",
    "# Create a chatbot Question & Answer chain from the retriever\n",
    "#chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "#    chat_api, chain_type=\"stuff\", retriever=retriever\n",
    "#)\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    chat_api, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    verbose=True, \n",
    "    chain_type_kwargs={\"prompt\": prompt, \"verbose\": True}\n",
    ")\n",
    "\n",
    "chain_traditional = prettifyChain(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\ntext: The injured Player must wait until their substitute has been released from the Penalty Box before they are eligible to play. If, however, there is a stoppage of play prior to the expiration of their penalty, they must then replace their Teammate in the Penalty Box and is then eligible to return once their penalty has expired.', metadata={'source': '', 'path': 'iihf_official_rulebook_2023/24', 'chunkId': '44983ccbe437e23d4b6bcaedcf95663fb945ce45', 'documentUri': '/data-transfer/iihf/rulebook.md', 'chunkSeqId': 56, 'header4': '8.1. INJURED PLAYER', 'header3': 'RULE 8 INJURED PLAYERS', 'header2': 'SECTION 02 TEAMS', 'header1': 'IIHF Official Rulebook 2023/24'})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = retriever.invoke(\"what happens if player is injured\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask some questions\n",
    "\n",
    "Finally, you can use the Langchain chain, which combines the retriever\n",
    "and the vector store into a nice question and answer interface.\n",
    "\n",
    "You can see both the answer and the source that the answer came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: [INST]<<SYS>> You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.<</SYS>> \n",
      "Question: what is the size of the rink? \n",
      "Context: \n",
      "text: The official size of the Rink shall be 60 m long and 26 m to 30 m wide. The corners shall be rounded in the arc of a circle with a radius of 7.0 m to 8.50 m. Any deviations from these dimensions for any IIHF competition require IIHF approval. \n",
      "Answer: [/INST]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "[/INST] The size of the rink is 60 meters long and 26-30 meters wide.\n"
     ]
    }
   ],
   "source": [
    "chain_traditional(\"what is the size of the rink?\")\n",
    "#chain_traditional(\"what happens if player is injured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expand - connect the chunks into linked lists to allwo search over windows of chunks\n",
    "\n",
    "You can now create relationships between all nodes in that list of chunks, effectively creating a linked list from the\n",
    "first chunk to the last.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Collect all the form IDs and form 10k item names\n",
    "distinct_path_result = gdb.execute_query(\"\"\"\n",
    "MATCH (c:Chunk) RETURN DISTINCT c.path as path\n",
    "\"\"\").records\n",
    "\n",
    "distinct_path_list = list(map(lambda x: x['path'], distinct_path_result))\n",
    "\n",
    "# Connect *all* section chunks into a linked list..\n",
    "cypher = \"\"\"\n",
    "  MATCH (from_same_path:Chunk) // match all chunks\n",
    "  WHERE from_same_path.path = $path // where the chunks are from the same path\n",
    "  WITH from_same_path // with those collections of chunks\n",
    "    ORDER BY from_same_path.chunkSeqId ASC // order the chunks by their sequence ID\n",
    "  WITH collect(from_same_path) as same_path_chunk_list // collect the chunks into a list\n",
    "    CALL apoc.nodes.link(same_path_chunk_list, \"NEXT\", {avoidDuplicates: true}) // then create a linked list in the graph\n",
    "  RETURN size(same_path_chunk_list)\n",
    "\"\"\"\n",
    "\n",
    "for path in distinct_path_list:\n",
    "    gdb.execute_query(cypher, \n",
    "             path=path\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search with Query Window\n",
    "\n",
    "You can now create a question answering chain.\n",
    "\n",
    "The default Neo4jVector uses a basic cypher query to peform vector similarity search.\n",
    "\n",
    "That query can be extended to do whatever you want in a Cypher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Cypher query extension will receive two variables: node and score and it should should return three fields: text, score, and metadata.\n",
    "retrieval_query_window = \"\"\"\n",
    " OPTIONAL MATCH window=\n",
    "    (:Chunk)-[:NEXT*0..1]->(node)-[:NEXT*0..1]->(:Chunk)\n",
    "WITH node, score, window as longestWindow \n",
    "  ORDER BY node,  length(window) DESC\n",
    "WITH nodes(longestWindow) as chunkList, node, score\n",
    "  UNWIND chunkList as chunkRows\n",
    "WITH collect(chunkRows.text) as textList, node, score\n",
    "WITH apoc.text.join(textList, \" \\n \") as text,\n",
    "    score,\n",
    "    node {.source} AS metadata \n",
    "RETURN text, score, metadata  ORDER BY score DESC LIMIT 1 \n",
    "\"\"\"\n",
    "\n",
    "vector_store_window = Neo4jVector.from_existing_index(\n",
    "    embedding=embeddings_api,\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,\n",
    "    database=\"neo4j\",\n",
    "    index_name=VECTOR_INDEX_NAME,\n",
    "    text_node_property=VECTOR_SOURCE_PROPERTY,\n",
    "    retrieval_query=retrieval_query_window\n",
    ")\n",
    "\n",
    "# Create a retriever from the vector store\n",
    "retriever_window = vector_store_window.as_retriever(search_kwargs={'k': 1})\n",
    "\n",
    "# Create a chatbot Question & Answer chain from the retriever\n",
    "chain_window = prettifyChain(RetrievalQA.from_chain_type(\n",
    "    chat_api, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever_window,\n",
    "    chain_type_kwargs={\"verbose\": True}\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever_window.invoke(\"what happens if player is injured\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_window(\"what happens if player is injured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_query_window = \"\"\"\n",
    " OPTIONAL MATCH window=\n",
    "    (:Chunk)-[:NEXT*0..1]->(node)-[:NEXT*0..1]->(:Chunk)\n",
    "WITH node, score, window as longestWindow \n",
    "  ORDER BY node,  length(window) DESC\n",
    "WITH nodes(longestWindow) as chunkList, node, score\n",
    "  UNWIND chunkList as chunkRows\n",
    "WITH collect(chunkRows.text) as textList, node, score\n",
    "WITH apoc.text.join(textList, \" \\n \") as text,\n",
    "    score,\n",
    "    node {.source} AS metadata \n",
    "RETURN text, score, metadata  ORDER BY score DESC LIMIT 1 \n",
    "\"\"\"\n",
    "\n",
    "def neo4j_vector_search_2(question, retrieval_query):\n",
    "  \"\"\"Search for similar nodes using the Neo4j vector index\"\"\"\n",
    "  vector_search_query = \"\"\"\n",
    "    CALL db.index.vector.queryNodes($index_name, $top_k, $question_embedding) \n",
    "        YIELD node, score\n",
    "  \"\"\" + retrieval_query\n",
    "  similar = []\n",
    "\n",
    "  print (\"Using vector index: \" + str(VECTOR_INDEX_NAME))\n",
    "    \n",
    "  question_embedding = embeddings_api.embed_query(question)\n",
    "  return gdb.execute_query(vector_search_query,\n",
    "                      question=question, \n",
    "                      question_embedding=question_embedding, \n",
    "                      index_name=VECTOR_INDEX_NAME, \n",
    "                      top_k=5\n",
    "                    ).records\n",
    "\n",
    "search_results = neo4j_vector_search_2(\n",
    "    'what happens if player is injured', retrieval_query_window\n",
    ")\n",
    "search_results[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_embedding = embeddings_api.embed_query('what happens if player is injured')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(question_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
