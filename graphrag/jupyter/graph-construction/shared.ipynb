{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Shared Imports, Configuration and Helpers\n",
    "\n",
    "Common libraries and configurations for the other notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports, constants, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv -ov ../.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat ../.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Neo4j at bolt://neo4j-1:7687 as neo4j\n",
      "Embedding with ollama using mxbai-embed-large\n",
      "Chatting with ollama using llama3\n"
     ]
    }
   ],
   "source": [
    "# import python libraries\n",
    "import os\n",
    "\n",
    "# Common data processing\n",
    "import json\n",
    "import textwrap\n",
    "\n",
    "# import hash functionality\n",
    "import hashlib\n",
    "\n",
    "# Neo4j driver\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Langchain\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Lancgchain for Ollama (for hosting a local LLM)\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "# Langchain for OpenAI (which requires an API key and internet connection)\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Use Langchain Hub (to get prompts)\n",
    "from langchain import hub\n",
    "\n",
    "# Load from environment\n",
    "NEO4J_URI = os.getenv('NEO4J_URI')\n",
    "NEO4J_USERNAME = os.getenv('NEO4J_USERNAME')\n",
    "NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')\n",
    "NEO4J_DATABASE = os.getenv('NEO4J_DATABASE') or 'neo4j'\n",
    "\n",
    "# Global constants\n",
    "VECTOR_INDEX_NAME = 'chunks_vector'\n",
    "VECTOR_NODE_LABEL = 'Chunk'\n",
    "VECTOR_SOURCE_PROPERTY = 'text'\n",
    "VECTOR_EMBEDDING_PROPERTY = 'embedding'\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "EMBEDDING_API = os.getenv('EMBEDDING_API') or 'openai'\n",
    "EMBEDDING_MODEL = os.getenv('EMBEDDING_MODEL') or 'text-embedding-ada-002'\n",
    "CHAT_API = os.getenv('CHAT_API') or 'openai'\n",
    "CHAT_MODEL = os.getenv('CHAT_MODEL') or 'gpt-3.5-turbo'\n",
    "OLLAMA_URL = os.getenv('OLLAMA_URL')\n",
    "\n",
    "# Data directory for Form 10-K json files and Form 13 csv file\n",
    "#ROOT_DIR = os.path.dirname(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "#DATA_DIR = f\"{ROOT_DIR}/{os.getenv('DATA_DIR') or 'data/single'}\"\n",
    "\n",
    "print(f\"Connecting to Neo4j at {NEO4J_URI} as {NEO4J_USERNAME}\")\n",
    "#print(f\"Using data from {DATA_DIR}\")\n",
    "print(f\"Embedding with {EMBEDDING_API} using {EMBEDDING_MODEL}\")\n",
    "print(f\"Chatting with {CHAT_API} using {CHAT_MODEL}\")\n",
    "\n",
    "gdb = GraphDatabase.driver(uri=NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "graph = Neo4jGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_api = None\n",
    "chat_api = None\n",
    "\n",
    "match EMBEDDING_API: \n",
    "  case \"ollama\": \n",
    "    embeddings_api = OllamaEmbeddings(base_url=OLLAMA_URL, model=EMBEDDING_MODEL, temperature=0, top_k=10, top_p=0.5)\n",
    "  case \"openai\": \n",
    "    if OPENAI_API_KEY is None:\n",
    "      raise ValueError(\"OpenAI_API_KEY is not set. Please add it to your .env file.\")\n",
    "    embeddings_api = OpenAIEmbeddings(api_key=OPENAI_API_KEY)\n",
    "match CHAT_API:\n",
    "  case \"ollama\": \n",
    "    chat_api = ChatOllama(base_url=OLLAMA_URL, model=CHAT_MODEL, temperature=0)\n",
    "  case \"openai\": \n",
    "    if OPENAI_API_KEY is None:\n",
    "      raise ValueError(\"OpenAI_API_KEY is not set. Please add it to your .env file.\")\n",
    "    chat_api = ChatOpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neo4j_vector_search(question, index_name):\n",
    "  \"\"\"Search for similar nodes using the Neo4j vector index\"\"\"\n",
    "  vector_search_query = \"\"\"\n",
    "    CALL db.index.vector.queryNodes($index_name, $top_k, $question_embedding) \n",
    "        YIELD node, score\n",
    "    RETURN score, node AS node\n",
    "  \"\"\"\n",
    "  similar = []\t\n",
    "\n",
    "  print (\"Using vector index: \" + index_name)\n",
    "    \n",
    "  question_embedding = embeddings_api.embed_query(question)\n",
    "  return gdb.execute_query(vector_search_query,\n",
    "                      question=question, \n",
    "                      question_embedding=question_embedding, \n",
    "                      index_name=index_name, \n",
    "                      top_k=10\n",
    "                    ).records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to pretty print the chain's response\n",
    "def prettifyChain(chain):\n",
    "  def prettychain(question:str):\n",
    "    response = chain({\"question\": question, \"query\":question}, return_only_outputs=True,)\n",
    "    answer = response['answer'] if ('answer' in response) else response['result']\n",
    "    print(textwrap.fill(answer, 80))\n",
    "  return prettychain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
